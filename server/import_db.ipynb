{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Get url from https://www.worldcubeassociation.org/export/results\n",
        "url = input()\n",
        "\n",
        "# If url contains .sql, replace with .tsv\n",
        "url = url.replace('.sql', '.tsv')\n",
        "\n",
        "print(\"Download\")\n",
        "urllib.request.urlretrieve(url, \"WCA_export.zip\")\n",
        "\n",
        "if os.path.exists('WCA_export'):\n",
        "    shutil.rmtree('WCA_export')\n",
        "\n",
        "print(\"Unzip\")\n",
        "shutil.unpack_archive('WCA_export.zip', 'WCA_export')\n",
        "\n",
        "print(\"Rename\")\n",
        "for filename in os.listdir('WCA_export'):\n",
        "    # Remove WCA_export_ from the filename\n",
        "    new_name = filename.replace('WCA_export_', '')\n",
        "    os.rename(f'WCA_export/{filename}', f'WCA_export/{new_name}')\n",
        "\n",
        "print(\"Remove unnecessary\")\n",
        "\n",
        "def remove_if_exists(path):\n",
        "    if os.path.exists(path):\n",
        "        os.remove(path)\n",
        "\n",
        "remove_if_exists('./WCA_export/championships.tsv')\n",
        "remove_if_exists('./WCA_export/eligible_country_iso2s_for_championship.tsv')\n",
        "remove_if_exists('./WCA_export/formats.tsv')\n",
        "remove_if_exists('./WCA_export/round_types.tsv')\n",
        "remove_if_exists('./WCA_export/scrambles.tsv')\n",
        "\n",
        "# Legacy export names\n",
        "remove_if_exists('./WCA_export/Formats.tsv')\n",
        "remove_if_exists('./WCA_export/RoundTypes.tsv')\n",
        "remove_if_exists('./WCA_export/Scrambles.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "export_dir = 'WCA_export'\n",
        "\n",
        "# Copy staff.tsv to export_dir if it exists locally\n",
        "local_staff_candidates = ['staff.tsv', 'Staff.tsv']\n",
        "export_staff_path = os.path.join(export_dir, 'staff.tsv')\n",
        "for candidate in local_staff_candidates:\n",
        "    if os.path.exists(candidate):\n",
        "        shutil.copy(candidate, export_staff_path)\n",
        "        break\n",
        "\n",
        "filenames = os.listdir(export_dir)\n",
        "filenames = [f for f in filenames if f.endswith('.tsv')]\n",
        "\n",
        "def normalize_tablename(name):\n",
        "    lower = name.lower()\n",
        "    legacy_map = {\n",
        "        'rankssingle': 'ranks_single',\n",
        "        'ranksaverage': 'ranks_average',\n",
        "        'roundtypes': 'round_types',\n",
        "    }\n",
        "    return legacy_map.get(lower, lower)\n",
        "\n",
        "dfs = {}\n",
        "\n",
        "def normalize_columns(tablename, df):\n",
        "    # Convert snake_case export columns to existing camelCase expectations\n",
        "    if tablename in ['ranks_single', 'ranks_average']:\n",
        "        df = df.rename(columns={\n",
        "            'event_id': 'eventId',\n",
        "            'person_id': 'personId',\n",
        "            'country_id': 'countryId',\n",
        "            'continent_id': 'continentId',\n",
        "            'world_rank': 'worldRank',\n",
        "            'continent_rank': 'continentRank',\n",
        "            'country_rank': 'countryRank',\n",
        "        })\n",
        "    elif tablename == 'results':\n",
        "        df = df.rename(columns={\n",
        "            'competition_id': 'competitionId',\n",
        "            'round_type_id': 'roundTypeId',\n",
        "            'event_id': 'eventId',\n",
        "            'person_id': 'personId',\n",
        "            'person_country_id': 'personCountryId',\n",
        "            'person_name': 'personName',\n",
        "            'format_id': 'formatId',\n",
        "            'regional_single_record': 'regionalSingleRecord',\n",
        "            'regional_average_record': 'regionalAverageRecord',\n",
        "        })\n",
        "    elif tablename == 'persons':\n",
        "        df = df.rename(columns={\n",
        "            'country_id': 'countryId',\n",
        "            'continent_id': 'continentId',\n",
        "        })\n",
        "    elif tablename == 'countries':\n",
        "        df = df.rename(columns={'continent_id': 'continentId'})\n",
        "    return df\n",
        "\n",
        "for filename in filenames:\n",
        "    tablename = normalize_tablename(filename.split('.')[0])\n",
        "    df = pd.read_csv(f'{export_dir}/{filename}', delimiter='\\t')\n",
        "    dfs[tablename] = normalize_columns(tablename, df)\n",
        "    print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Remove unnecessary columns')\n",
        "\n",
        "# Normalize persons columns for WCA export v2\n",
        "if 'id' in dfs['persons'].columns and 'wca_id' not in dfs['persons'].columns:\n",
        "    dfs['persons'] = dfs['persons'].rename(columns={'id': 'wca_id'})\n",
        "if 'subid' in dfs['persons'].columns and 'sub_id' not in dfs['persons'].columns:\n",
        "    dfs['persons'] = dfs['persons'].rename(columns={'subid': 'sub_id'})\n",
        "\n",
        "dfs['persons'] = dfs['persons'].drop(columns=['sub_id'], errors='ignore')\n",
        "\n",
        "results_drop_columns = [\n",
        "    'personName', 'formatId', 'personCountryId',\n",
        "    'person_name', 'format_id', 'person_country_id',\n",
        "    'value1', 'value2', 'value3', 'value4', 'value5',\n",
        "]\n",
        "dfs['results'] = dfs['results'].drop(columns=results_drop_columns, errors='ignore')\n",
        "dfs['results'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Cast event ids to string')\n",
        "dfs['events'].id = dfs['events'].id.astype(str)\n",
        "dfs['ranks_single'].eventId = dfs['ranks_single'].eventId.astype(str)\n",
        "dfs['ranks_average'].eventId = dfs['ranks_average'].eventId.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Handle duplicate persons')\n",
        "\n",
        "# If a person has moved countries, then they could have multiple entries\n",
        "dfs['persons'].drop_duplicates('wca_id', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Populate country ids')\n",
        "country_ids = dfs['persons'][['wca_id', 'countryId']]\n",
        "\n",
        "dfs['ranks_single'] = dfs['ranks_single'].merge(country_ids, left_on='personId', right_on='wca_id').drop('wca_id', axis=1)\n",
        "dfs['ranks_average'] = dfs['ranks_average'].merge(country_ids, left_on='personId', right_on='wca_id').drop('wca_id', axis=1)\n",
        "dfs['ranks_single']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Populate continent ids')\n",
        "continent_ids = dfs['countries'][['id', 'continentId']]\n",
        "\n",
        "dfs['ranks_single'] = dfs['ranks_single'].merge(continent_ids, left_on='countryId', right_on='id').drop('id', axis=1)\n",
        "dfs['ranks_average'] = dfs['ranks_average'].merge(continent_ids, left_on='countryId', right_on='id').drop('id', axis=1)\n",
        "dfs['persons'] = dfs['persons'].merge(continent_ids, left_on='countryId', right_on='id', suffixes=('', '_drop')).drop('id_drop', axis=1, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Populate names')\n",
        "names = dfs['persons'][['wca_id', 'name']]\n",
        "\n",
        "dfs['ranks_single'] = dfs['ranks_single'].merge(names, left_on='personId', right_on='wca_id').drop('wca_id', axis=1)\n",
        "dfs['ranks_average'] = dfs['ranks_average'].merge(names, left_on='personId', right_on='wca_id').drop('wca_id', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Calculate max ranks')\n",
        "single_world_maxes = dfs['ranks_single'].groupby('eventId').max()['worldRank']\n",
        "average_world_maxes = dfs['ranks_average'].groupby('eventId').max()['worldRank']\n",
        "\n",
        "single_continent_maxes = dfs['ranks_single'].groupby(['continentId', 'eventId']).max()['continentRank']\n",
        "average_continent_maxes = dfs['ranks_average'].groupby(['continentId', 'eventId']).max()['continentRank']\n",
        "\n",
        "single_country_maxes = dfs['ranks_single'].groupby(['countryId', 'eventId']).max()['countryRank']\n",
        "average_country_maxes = dfs['ranks_average'].groupby(['countryId', 'eventId']).max()['countryRank']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# People who switched nationality may have rank of 0 for continentRank and countryRank.\n",
        "# Set these values to the max possible value for that event.\n",
        "\n",
        "def fill_zero_ranks(df, continent_maxes, country_maxes):\n",
        "  continent_lookup = df[['continentId', 'eventId']].apply(tuple, axis=1).map(continent_maxes)\n",
        "  df.loc[df['continentRank'] == 0, 'continentRank'] = continent_lookup[df['continentRank'] == 0].values\n",
        "\n",
        "  country_lookup = df[['countryId', 'eventId']].apply(tuple, axis=1).map(country_maxes)\n",
        "  df.loc[df['countryRank'] == 0, 'countryRank'] = country_lookup[df['countryRank'] == 0].values\n",
        "\n",
        "  return df\n",
        "\n",
        "dfs['ranks_single'] = fill_zero_ranks(dfs['ranks_single'], single_continent_maxes, single_country_maxes)\n",
        "dfs['ranks_average'] = fill_zero_ranks(dfs['ranks_average'], average_continent_maxes, average_country_maxes)\n",
        "\n",
        "print('There should be very few rows where continentRank or countryRank is 0')\n",
        "dfs['ranks_single'][dfs['ranks_single'].countryRank == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def calculate_sum_of_ranks(rank_type, events, world_maxes, continent_maxes, country_maxes):\n",
        "\n",
        "    person_ids = dfs['persons'].wca_id.unique()\n",
        "    combinations = pd.DataFrame(list(itertools.product(person_ids, events)), columns=['personId', 'eventId'])\n",
        "\n",
        "    ranks_table = dfs[f'ranks_{rank_type}']\n",
        "\n",
        "    # Make a row for every person and every event. If person has no result, their rank is NaN\n",
        "    ranks_all = combinations \\\n",
        "        .merge(ranks_table[['personId', 'eventId', 'worldRank', 'continentRank', 'countryRank']], on=['personId', 'eventId'], how='left') \\\n",
        "        .merge(dfs['persons'][['wca_id', 'countryId', 'continentId']], left_on='personId', right_on='wca_id', how='left')\n",
        "\n",
        "    ranks_all['worldRank'] = ranks_all['worldRank'].fillna(ranks_all['eventId'].map(world_maxes))\n",
        "    ranks_all['continentRank'] = ranks_all['continentRank'].fillna(ranks_all.set_index(['continentId', 'eventId']).index.map(continent_maxes).to_series(index=ranks_all.index))\n",
        "    ranks_all['countryRank'] = ranks_all['countryRank'].fillna(ranks_all.set_index(['countryId', 'eventId']).index.map(country_maxes).to_series(index=ranks_all.index))\n",
        "\n",
        "    sor = ranks_all.groupby('personId')[['worldRank', 'continentRank', 'countryRank']].sum()\n",
        "    sor = sor.reset_index()\n",
        "    sor = sor.rename(columns={\n",
        "        'worldRank': f'worldSor{rank_type.title()}',\n",
        "        'continentRank': f'continentSor{rank_type.title()}',\n",
        "        'countryRank': f'countrySor{rank_type.title()}',\n",
        "    })\n",
        "    return sor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_events = ['222', '333', '333bf', '333fm', '333mbf', '333oh', '444', '444bf', '555', '555bf', '666', '777', 'clock', 'minx', 'pyram', 'skewb', 'sq1']\n",
        "\n",
        "# Same as single_events but no 333mbf\n",
        "average_events = ['222', '333', '333bf', '333fm', '333oh', '444', '444bf', '555', '555bf', '666', '777', 'clock', 'minx', 'pyram', 'skewb', 'sq1']\n",
        "\n",
        "print('Single sum of ranks')\n",
        "single_sor = calculate_sum_of_ranks('single', single_events, single_world_maxes, single_continent_maxes, single_country_maxes)\n",
        "print('Average sum of ranks')\n",
        "average_sor = calculate_sum_of_ranks('average', average_events, average_world_maxes, average_continent_maxes, average_country_maxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs['persons'] = dfs['persons'].merge(single_sor, left_on='wca_id', right_on='personId').drop('personId', axis=1)\n",
        "dfs['persons'] = dfs['persons'].merge(average_sor, left_on='wca_id', right_on='personId').drop('personId', axis=1)\n",
        "dfs['persons'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the min rank instead of rank 1, because sometimes rank 1 doesn't exist (idk why)\n",
        "best_singles = dfs['ranks_single'].sort_values('worldRank').drop_duplicates('eventId')[['eventId', 'best']].rename(columns={'best': 'single'})\n",
        "best_averages = dfs['ranks_average'].sort_values('worldRank').drop_duplicates('eventId')[['eventId', 'best']].rename(columns={'best': 'average'})\n",
        "\n",
        "world_bests = best_singles.merge(best_averages, on='eventId', how='outer')\n",
        "world_bests = {eventId: (single, average) for eventId, single, average in world_bests.values}\n",
        "world_bests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def group_dict(d):\n",
        "    '''\n",
        "    Example input: {\n",
        "        ('a', 'b'): 1,\n",
        "        ('a', 'c'): 2,\n",
        "    }\n",
        "\n",
        "    Example output: {\n",
        "        'a': {\n",
        "            'b': 1,\n",
        "            'c': 2,\n",
        "        },\n",
        "    }\n",
        "    '''\n",
        "    output = {}\n",
        "    for k in d:\n",
        "        if k[0] not in output:\n",
        "            output[k[0]] = {}\n",
        "        \n",
        "        output[k[0]][k[1]] = d[k]\n",
        "    return output\n",
        "\n",
        "best_singles = dfs['ranks_single'].sort_values('continentRank').drop_duplicates(['eventId', 'continentId'])[['eventId', 'best', 'continentId']].rename(columns={'best': 'single'})\n",
        "best_averages = dfs['ranks_average'].sort_values('continentRank').drop_duplicates(['eventId', 'continentId'])[['eventId', 'best', 'continentId']].rename(columns={'best': 'average'})\n",
        "\n",
        "continent_bests = best_singles.merge(best_averages, on=['eventId', 'continentId'], how='outer').drop_duplicates()\n",
        "continent_bests = {(continentId, eventId): (single, average) for eventId, single, continentId, average in continent_bests.values}\n",
        "continent_bests = group_dict(continent_bests)\n",
        "\n",
        "best_singles = dfs['ranks_single'].sort_values('countryRank').drop_duplicates(['eventId', 'countryId'])[['eventId', 'best', 'countryId']].rename(columns={'best': 'single'})\n",
        "best_averages = dfs['ranks_average'].sort_values('countryRank').drop_duplicates(['eventId', 'countryId'])[['eventId', 'best', 'countryId']].rename(columns={'best': 'average'})\n",
        "\n",
        "country_bests = best_singles.merge(best_averages, on=['eventId', 'countryId'], how='outer').drop_duplicates()\n",
        "country_bests = {(continentId, eventId): (single, average) for eventId, single, continentId, average in country_bests.values}\n",
        "country_bests = group_dict(country_bests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_rank_dict(tablename):\n",
        "    d = {}\n",
        "    for row in dfs[tablename][['personId', 'eventId', 'best']].values:\n",
        "        personId, eventId, best = row\n",
        "\n",
        "        if personId not in d:\n",
        "            d[personId] = {}\n",
        "\n",
        "        d[personId][eventId] = best\n",
        "    \n",
        "    # Make sure every person at least has an empty object\n",
        "    for id in dfs['persons']['wca_id']:\n",
        "        if id not in d:\n",
        "            d[id] = {}\n",
        "\n",
        "    return d\n",
        "\n",
        "print('Build rank dicts')\n",
        "single_dict = build_rank_dict('ranks_single')\n",
        "average_dict = build_rank_dict('ranks_average')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def mbldScore(value):\n",
        "    if not value:\n",
        "        return 0\n",
        "    seconds = math.floor(value / 100) % 1e5\n",
        "    points = 99 - (math.floor(value / 1e7) % 100)\n",
        "    centiseconds = None if seconds == 99999 else seconds * 100\n",
        "    proportionOfHourLeft = 1 - centiseconds / 360000\n",
        "    score = points + proportionOfHourLeft\n",
        "    return max(score, 0)\n",
        "\n",
        "def get_kinch_score(personId, bests, key):\n",
        "    if key:\n",
        "        if key in bests:\n",
        "            bests = bests[key]\n",
        "        else:\n",
        "            # This edge case can occur if, for example, a person moves to a new country that has no results.\n",
        "            # This occurred for wca id 2018YEDD01 who moved to Barbados!\n",
        "            return 0\n",
        "    scores = []\n",
        "\n",
        "    # Handle 333mbf\n",
        "    single = single_dict[personId].get(\"333mbf\")\n",
        "    average = average_dict[personId].get(\"333mbf\")\n",
        "    bestSingle, bestAverage = bests[\"333mbf\"] if \"333mbf\" in bests else (None, None)\n",
        "\n",
        "    mbldPersonal = mbldScore(single)\n",
        "    mbldRecord = mbldScore(bestSingle)\n",
        "\n",
        "    if mbldRecord:\n",
        "        scores.append(mbldPersonal / mbldRecord * 100)\n",
        "    else:\n",
        "        # If nobody has mbld, use 100\n",
        "        scores.append(0)\n",
        "\n",
        "    # For these events, use better between single and average\n",
        "    for eventId in [\"333fm\", \"333bf\", \"444bf\", \"555bf\"]:\n",
        "        single = single_dict[personId].get(eventId)\n",
        "        average = average_dict[personId].get(eventId)\n",
        "        bestSingle, bestAverage = bests[eventId] if eventId in bests else (None, None)\n",
        "\n",
        "        if not single and not average:\n",
        "            scores.append(0)\n",
        "        elif not bestSingle or not bestAverage:\n",
        "            # This can happen if a person has multiple countryIds and one of the countries has no result for the event.\n",
        "            scores.append(100)\n",
        "        elif not average:\n",
        "            # If no average, use single\n",
        "            scores.append(bestSingle / single * 100)\n",
        "        else:\n",
        "            # # If there is an average, use the better of the two\n",
        "            scores.append(max(\n",
        "                bestSingle / single * 100,\n",
        "                bestAverage / average * 100\n",
        "            ))\n",
        "\n",
        "    # For these events, use average\n",
        "    for eventId in ['222', '333', '333oh', '444', '555', '666', '777', 'clock', 'minx', 'pyram', 'skewb', 'sq1']:\n",
        "        single = single_dict[personId].get(eventId)\n",
        "        average = average_dict[personId].get(eventId)\n",
        "        bestSingle, bestAverage = bests[eventId] if eventId in bests else (None, None)\n",
        "\n",
        "        if not average:\n",
        "            scores.append(0)\n",
        "        elif not bestAverage:\n",
        "            # This can happen if a person has multiple countryIds and one of the countries has no result for the event.\n",
        "            scores.append(100)\n",
        "        else:\n",
        "            scores.append(bestAverage / average * 100)\n",
        "\n",
        "    avgScore = sum(scores) / len(scores)\n",
        "    if personId == '2011BANS02':\n",
        "        print(\"scores\", scores)\n",
        "        print(\"avgScore\", avgScore)\n",
        "    return avgScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "persons = dfs['persons']\n",
        "\n",
        "print('World kinch')\n",
        "persons['worldKinch'] = persons.apply(lambda row: get_kinch_score(row['wca_id'], world_bests, None), axis=1)\n",
        "\n",
        "print('Continent kinch')\n",
        "persons['continentKinch'] = persons.apply(lambda row: get_kinch_score(row['wca_id'], continent_bests, row['continentId']), axis=1)\n",
        "\n",
        "print('Country kinch')\n",
        "persons['countryKinch'] = persons.apply(lambda row: get_kinch_score(row['wca_id'], country_bests, row['countryId']), axis=1)\n",
        "\n",
        "persons.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Populate startDate and endDate')\n",
        "\n",
        "# Format is yyyy-mm-dd\n",
        "# Pad month and day with zeros\n",
        "dfs['competitions']['startDate'] = dfs['competitions'].apply(lambda row: f\"{row['year']}-{str(row['month']).zfill(2)}-{str(row['day']).zfill(2)}\", axis=1)\n",
        "dfs['competitions']['endDate'] = dfs['competitions'].apply(lambda row: f\"{row['year']}-{str(row['end_month']).zfill(2)}-{str(row['end_day']).zfill(2)}\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Calculate birthdays')\n",
        "\n",
        "# Get competition data\n",
        "comps = dfs['results'][['competitionId', 'personId']].drop_duplicates()\n",
        "comps = comps.merge(dfs['competitions'][['id', 'startDate']], left_on='competitionId', right_on='id').drop('id', axis=1)\n",
        "comps = comps.merge(dfs['persons'][['wca_id', 'name']], left_on='personId', right_on='wca_id').drop('wca_id', axis=1)\n",
        "comps = comps.sort_values('startDate')\n",
        "\n",
        "# Get first comp for each person\n",
        "first_comps = {}\n",
        "for row in comps.values:\n",
        "    personId = row[1]\n",
        "\n",
        "    if personId in first_comps:\n",
        "        continue\n",
        "\n",
        "    first_comps[personId] = row\n",
        "\n",
        "def sort_dict(d, keys):\n",
        "    output = {}\n",
        "    for key in keys:\n",
        "        output[key] = d[key]\n",
        "    return output\n",
        "\n",
        "# Get persons in order of rank\n",
        "persons = dfs['ranks_single'].sort_values('worldRank')['personId'].unique()\n",
        "\n",
        "first_comps = sort_dict(first_comps, persons)\n",
        "\n",
        "dfs['birthdays'] = pd.DataFrame(first_comps.values(), columns=['competitionId', 'personId', 'date', 'name'])\n",
        "dfs['birthdays'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(f'{export_dir}/metadata.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "\n",
        "    dfs['miscellaneous'] = pd.DataFrame({\n",
        "        'key': ['export_date'],\n",
        "        'value': [data['export_date'][0:10]], # Only first 10 chars for yyyy-mm-dd\n",
        "    })\n",
        "\n",
        "dfs['miscellaneous']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to old format for backwards compatibility\n",
        "\n",
        "Changelog can be found at: https://www.worldcubeassociation.org/export/results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = dfs[\"results\"]\n",
        "attempts = dfs[\"result_attempts\"]\n",
        "\n",
        "# Pivot attempts back into value1â€“value5\n",
        "attempts_wide = (\n",
        "    attempts\n",
        "    .pivot(index=\"result_id\", columns=\"attempt_number\", values=\"value\")\n",
        "    .rename(columns=lambda x: f\"value{x}\")\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Merge back into results\n",
        "results_old = results.merge(\n",
        "    attempts_wide,\n",
        "    left_on=\"id\",\n",
        "    right_on=\"result_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "results_old = results_old.drop(columns=[\"id\", \"result_id\"])\n",
        "\n",
        "competitions_old = dfs[\"competitions\"].rename(columns={\n",
        "    \"delegates\": \"wcaDelegate\",\n",
        "    \"organizers\": \"organiser\",\n",
        "    \"latitude_microdegrees\": \"latitude\",\n",
        "    \"longitude_microdegrees\": \"longitude\",\n",
        "    \"event_specs\": \"eventSpecs\",\n",
        "    \"end_month\": \"endMonth\",\n",
        "    \"end_day\": \"endDay\",\n",
        "})\n",
        "\n",
        "persons_old = dfs[\"persons\"].drop(columns=['id']).rename(columns={\n",
        "    \"wca_id\": \"id\",\n",
        "    \"sub_id\": \"subid\",\n",
        "})\n",
        "\n",
        "dfs = {\n",
        "    \"Results\": results_old,\n",
        "    \"Events\": dfs[\"events\"],\n",
        "    \"Persons\": persons_old,\n",
        "    \"Competitions\": competitions_old,\n",
        "    \"Countries\": dfs[\"countries\"],\n",
        "    \"Continents\": dfs[\"continents\"],\n",
        "    \"RanksSingle\": dfs[\"ranks_single\"],\n",
        "    \"RanksAverage\": dfs[\"ranks_average\"],\n",
        "    \"Staff\": dfs[\"staff\"],\n",
        "    \"Birthdays\": dfs[\"birthdays\"],\n",
        "    \"Miscellaneous\": dfs[\"miscellaneous\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "def df_to_sqlite(df, table_name):\n",
        "    conn = sqlite3.connect('wca.db')\n",
        "\n",
        "    try:\n",
        "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "        print(f\"{table_name} table created\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "# Remove wca.db\n",
        "if os.path.exists('wca.db'):\n",
        "    os.remove('wca.db')\n",
        "\n",
        "for name in dfs:\n",
        "    df_to_sqlite(dfs[name], name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Create indices')\n",
        "conn = sqlite3.connect('wca.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# TODO: Analyze which of these indices are actually needed\n",
        "c.execute('CREATE INDEX idx_persons_wca_id ON Persons(id);')\n",
        "c.execute('CREATE INDEX idx_persons_countryId ON Persons(countryId);')\n",
        "c.execute('CREATE INDEX idx_persons_continentId ON Persons(continentId);')\n",
        "c.execute('CREATE INDEX idx_persons_countryKinch ON Persons(countryKinch);')\n",
        "c.execute('CREATE INDEX idx_persons_continentKinch ON Persons(continentKinch);')\n",
        "c.execute('CREATE INDEX idx_persons_worldKinch ON Persons(worldKinch);')\n",
        "c.execute('CREATE INDEX idx_persons_countrySorSingle ON Persons(countrySorSingle);')\n",
        "c.execute('CREATE INDEX idx_persons_continentSorSingle ON Persons(continentSorSingle);')\n",
        "c.execute('CREATE INDEX idx_persons_worldSorSingle ON Persons(worldSorSingle);')\n",
        "c.execute('CREATE INDEX idx_persons_countrySorAverage ON Persons(countrySorAverage);')\n",
        "c.execute('CREATE INDEX idx_persons_continentSorAverage ON Persons(continentSorAverage);')\n",
        "c.execute('CREATE INDEX idx_persons_worldSorAverage ON Persons(worldSorAverage);')\n",
        "c.execute('CREATE INDEX idx_ranks_single_eventId ON RanksSingle(eventId);')\n",
        "c.execute('CREATE INDEX idx_ranks_single_personId ON RanksSingle(personId);')\n",
        "c.execute('CREATE INDEX idx_ranks_single_worldRank ON RanksSingle(worldRank);')\n",
        "c.execute('CREATE INDEX idx_ranks_single_continentRank ON RanksSingle(continentRank);')\n",
        "c.execute('CREATE INDEX idx_ranks_single_countryRank ON RanksSingle(countryRank);')\n",
        "c.execute('CREATE INDEX idx_ranks_average_eventId ON RanksAverage(eventId);')\n",
        "c.execute('CREATE INDEX idx_ranks_average_personId ON RanksAverage(personId);')\n",
        "c.execute('CREATE INDEX idx_ranks_average_worldRank ON RanksAverage(worldRank);')\n",
        "c.execute('CREATE INDEX idx_ranks_average_continentRank ON RanksAverage(continentRank);')\n",
        "c.execute('CREATE INDEX idx_ranks_average_countryRank ON RanksAverage(countryRank);')\n",
        "c.execute('CREATE INDEX idx_staff_wca_id ON Staff(wca_id);')\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
